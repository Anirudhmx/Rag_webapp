# 🤖 RAG-Based File QA Web App

A **Retrieval-Augmented Generation (RAG)** web app built with **Streamlit** that allows users to upload documents (PDF, TXT, CSV), ask questions based on their content, and get answers generated by configurable LLMs like **GPT-2** and **Ollama (llama2:7b)**. Includes an evaluation dashboard to track performance, top-k similarity, and response latency.

---

## 🚀 Features

- 🔍 **Document Ingestion**: Supports PDF, TXT, CSV with text extraction, chunking, and vector embedding.
- 🤖 **Dynamic LLM Selection**: Choose between **GPT-2** and **Gemma (Ollama)** for response generation.
- 📈 **Evaluation Dashboard**: Tracks time taken per query, average top-k similarity, and model performance metrics.
- 🧠 **Semantic Retrieval**: Embedding-based cosine similarity search using `sentence-transformers`.
- 🌐 **End-to-End Interface**: Seamless Q&A experience with real-time feedback and expandable chat history.

---

## 🧠 Models Used

| Component          | Model Name           | Library/Source                    |
|--------------------|----------------------|-----------------------------------|
| Embeddings         | `all-MiniLM-L6-v2`   | [Sentence Transformers](https://www.sbert.net) |
| Generator (Local)  | `gpt2`               | [Hugging Face Transformers](https://huggingface.co/transformers) |
| Generator (LLM)    | `llama2:7b`           | [Ollama](https://ollama.com/)     |
| Similarity Metric  | Cosine Similarity    | Scikit-learn                      |

---

## 🛠️ Installation

```bash
# Clone the repo
git clone https://github.com/your-username/rag-file-qa-app.git
cd rag-file-qa-app

# Create and activate virtual environment
conda create -n ragbot python=3.10
conda activate ragbot

# Install dependencies
pip install -r requirements.txt

# If using Ollama, ensure Ollama is installed and run the model
ollama run gemma:2b
