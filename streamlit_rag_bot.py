import os
os.environ["TRANSFORMERS_NO_TF"] = "1"
import streamlit as st
import pandas as pd
import PyPDF2
from typing import List, Dict, Any
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import torch
import time 
from DocumentProcessor import DocumentProcessor
from RAGBot import RAGBot

# Page configuration
st.set_page_config(
    page_title="RAG Bot - Document Q&A",
    page_icon="ü§ñ",
    layout="wide"
)
page = st.sidebar.selectbox("Navigate", ["üìÅ Q&A Interface", "üìä Evaluation Stats"])

# Initialize session state
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = []
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'gpt2_model' not in st.session_state:
    st.session_state.gpt2_model = None
if 'gpt2_tokenizer' not in st.session_state:
    st.session_state.gpt2_tokenizer = None

# Initialize components
processor = DocumentProcessor()
bot = RAGBot()

# Sidebar for file upload and settings
st.sidebar.title("üìÅ Document Upload")

# Model loading status
with st.sidebar:
    st.write("### ü§ñ Model Status")
    model, tokenizer = bot.load_gpt2_model()
    if model is not None:
        st.success("‚úÖ GPT-2 model loaded successfully!")
    else:
        st.error("‚ùå Failed to load GPT-2 model")

# File upload
uploaded_files = st.sidebar.file_uploader(
    "Upload your documents",
    type=['pdf', 'txt', 'csv'],
    accept_multiple_files=True,
    help="Upload PDF, TXT, or CSV files"
)

# Process uploaded files
if uploaded_files:
    with st.sidebar:
        st.write("### Processing Files...")
        progress_bar = st.progress(0)
        
        new_documents = []
        new_embeddings = []
        
        for i, uploaded_file in enumerate(uploaded_files):
            file_name = uploaded_file.name
            file_type = file_name.split('.')[-1].lower()
            
            # Extract text based on file type
            if file_type == 'pdf':
                text = processor.extract_text_from_pdf(uploaded_file)
            elif file_type == 'txt':
                text = processor.extract_text_from_txt(uploaded_file)
            elif file_type == 'csv':
                text = processor.extract_text_from_csv(uploaded_file)
            else:
                st.error(f"Unsupported file type: {file_type}")
                continue
            
            if text:
                # Chunk the text
                chunks = processor.chunk_text(text)
                
                if chunks:
                    # Create embeddings
                    embeddings = processor.create_embeddings(chunks)
                    
                    # Store document info
                    doc_info = {
                        'name': file_name,
                        'type': file_type,
                        'chunks': chunks,
                        'text_preview': text[:200] + "..." if len(text) > 200 else text
                    }
                    
                    new_documents.append(doc_info)
                    new_embeddings.append(embeddings)
            
            progress_bar.progress((i + 1) / len(uploaded_files))
        
        # Update session state
        st.session_state.documents = new_documents
        st.session_state.embeddings = new_embeddings
        
        st.success(f"‚úÖ Processed {len(new_documents)} documents!")

# Display uploaded documents
if st.session_state.documents:
    st.sidebar.write("### üìö Uploaded Documents")
    for doc in st.session_state.documents:
        with st.sidebar.expander(f"üìÑ {doc['name']}"):
            st.write(f"**Type:** {doc['type'].upper()}")
            st.write(f"**Chunks:** {len(doc['chunks'])}")
            st.write(f"**Preview:** {doc['text_preview']}")

# Main interface
if page == "üìÅ Q&A Interface": 
    st.title("ü§ñ RAG Bot - Document Q&A")
    st.markdown("Upload your documents and ask questions about their content!")

    # Chat interface
    if st.session_state.documents:
        # Query input
        query = st.text_input("Ask a question about your documents:", 
                            placeholder="What is the main topic discussed in the documents?")
        
        if st.button("Ask", type="primary") and query:
            start = time.time()
            with st.spinner("Searching for relevant information..."):
                # Find relevant chunks
                relevant_chunks = bot.find_relevant_chunks(
                    query, 
                    st.session_state.documents, 
                    st.session_state.embeddings
                )
                # calculate similarity score stats
                processor = DocumentProcessor()
                query_embedding = processor.model.encode([query])
                all_embeddings = np.vstack(st.session_state.embeddings)
                similarities = cosine_similarity(query_embedding, all_embeddings)[0]
                top_k_indices = np.argsort(similarities)[-3:][::-1]
                top_similarities = [similarities[i] for i in top_k_indices]

                # Generate answer
                answer = bot.generate_answer(query, relevant_chunks)
                end = time.time()
                process_time = end-start
                st.write(f"Processing time to generate answer: {process_time}sec")
                # Add to chat history
                st.session_state.chat_history.append({
                    'question': query,
                    'answer': answer,
                    'relevant_chunks': relevant_chunks
                })
        
        # Display chat history
        if st.session_state.chat_history:
            st.write("## Chat History")
            
            for i, chat in enumerate(reversed(st.session_state.chat_history)):
                with st.expander(f"Q: {chat['question']}", expanded=(i==0)):
                    st.write("**Answer:**")
                    st.write(chat['answer'])
                    
                    if chat['relevant_chunks']:
                        st.write("** Source Context:**")
                        for j, chunk in enumerate(chat['relevant_chunks'][:2]):  # Show top 2 chunks
                            st.text_area(f"Relevant excerpt {j+1}:", chunk, height=100, disabled=True)
        
        # Clear chat button
        if st.button(" Clear Chat History"):
            st.session_state.chat_history = []
            st.rerun()

    else:
        # Welcome message
        st.info("üëÜ Please upload your documents using the sidebar to get started!")
        
        st.write("## How to use:")
        st.write("1. Upload PDF, TXT, or CSV files using the sidebar")
        st.write("2. Wait for GPT-2 model to load (first time only)")
        st.write("3. Ask questions about your documents")
        st.write("4. Get AI-generated answers based on document content")
        
        st.write("## Supported file types:")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(" **PDF** - Text extraction from PDF documents")
        with col2:
            st.write(" **TXT** - Plain text files")
        with col3:
            st.write(" **CSV** - Tabular data files")

    # Footer
    st.markdown("---")
    st.markdown("Built with Streamlit | Powered by GPT-2 & Sentence Transformers")
    st.markdown("**Models used:** GPT-2 (Text Generation) | all-MiniLM-L6-v2 (Embeddings) ")

elif page == "üìä Evaluation Stats":
    st.title("üìä Evaluation Metrics")

    if st.session_state.documents and st.session_state.embeddings:
        num_docs = len(st.session_state.documents)
        total_chunks = sum(len(doc['chunks']) for doc in st.session_state.documents)
        avg_chunks = total_chunks / num_docs

        # Compute avg similarity of most recent query
        if st.session_state.chat_history:
            last_query = st.session_state.chat_history[-1]['question']
            relevant_chunks = bot.find_relevant_chunks(
                last_query,
                st.session_state.documents,
                st.session_state.embeddings
            )

            processor = DocumentProcessor()
            query_embedding = processor.model.encode([last_query])
            all_embeddings = np.vstack(st.session_state.embeddings)
            similarities = cosine_similarity(query_embedding, all_embeddings)[0]
            top_k = np.argsort(similarities)[-3:][::-1]
            top_similarities = [similarities[i] for i in top_k]
            avg_similarity = np.mean(top_similarities)
        else:
            avg_similarity = None

        st.markdown(f"**Number of documents uploaded:** {num_docs}")
        st.markdown(f"**Average number of chunks per document:** {avg_chunks:.2f}")
        st.markdown(f"**Top-3 average similarity for last query:** {avg_similarity:.4f}" if avg_similarity else "**No query asked yet**")

    else:
        st.warning("Upload documents and ask a question in the Q&A section to see evaluation stats.")

